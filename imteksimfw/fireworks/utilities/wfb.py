# coding: utf-8
#
# wfb.py
#
# Copyright (C) 2020 IMTEK Simulation
# Author: Johannes Hoermann, johannes.hoermann@imtek.uni-freiburg.de
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
""" Workflow Builder, facilitates workflow constructionm """

__author__ = 'Johannes HÃ¶rmann'
__email__ = 'johannes.hoermann@imtek.uni-freiburg.de'
__copyright__ = 'Copyright 2019, University of Freiburg'

# IO, formats:
import logging, os.path
from tabulate import tabulate
import yaml
from jinja2 import Environment, FileSystemLoader
from jinja2 import meta, contextfunction
from fireworks import Firework, Workflow
from ansible.plugins.filter.core import to_yaml, to_nice_yaml, to_json, to_nice_json

# graphs and datastructures
import igraph
import numpy as np
from collections.abc import Iterable

# custom jinja2 filters
import time
def datetime(value,format='%Y-%m-%d-%H:%M'):
    if value == 'now':
        return time.strftime(format)
    else:
        return value.strftime(format)

@contextfunction
def get_context(c):
    return c

class WorkflowBuilder:

    @property
    def topological_order(self):
        return list(reversed(self.postorder()))

    @property
    def topological_position(self):
        """A list assigning its position in a topological ordering to a vertex"""
        return { v: i for i, v in enumerate(self.topological_order) }

    @property
    def maximum_distance(self):
        """A list assigning the largest possible path length from root to vertex"""
        return [(-1)*int(d) for d in self.g.shortest_paths(self.root, weights=-np.ones(len(self.g.es)))[0] ]

    @property
    def names(self):
        """Returns list of vertex names for list of indices"""
        return self.g.vs["name"]

    def positive_fw_id_generator(self):
        i = 0
        while True:
            yield 10*(i+1)
            i = i +1

    def __init__(self, system_file = None):
        self.logger = logging.getLogger(__name__)

        self.std_context         = {}
        self.persistent_contexts = {}
        self.transient_contexts  = {}
        self.dependencies        = {}
        self.metadata            = {}

        self.template_dir = 'templates'
        self.build_dir = 'build'

        self.g = igraph.Graph(directed=True)
        self.h = igraph.Graph(directed=True)

        self.t = None # degenerate tree

        self.topological_order = None

        self.root = None

        #  template engine
        self.env = None

        #logger = None

        # plot settings
        self.plt_layout = None
        self.plt_label_font_size = 10
        self.plt_bbox = (800, 600)
        self.plt_margin = 100

        if system_file is not None:
            self.read_system_file(system_file)
            self.build_graph()

    def read_system_file(self, system_file):
        assert type(system_file) is str

        with open(system_file) as stream:
            system = yaml.safe_load(stream)

        self.logger.info("File '{:s} contains the sections {}.".format( system_file, list(system.keys())) )
        assert "std"          in system
        assert "transient"    in system
        assert "persistent"   in system
        assert "dependencies" in system
        assert "name"         in system
        assert "metadata"     in system

        self.std_context         = system['std']
        self.persistent_contexts = system['persistent']
        self.transient_contexts  = system['transient']
        self.dependencies        = system['dependencies']
        self.name                = system["name"]
        self.metadata            = system["metadata"]

    def initialize_template_engine(self):
        self.env = Environment(
          loader=FileSystemLoader(self.template_dir),
          autoescape = False,
          extensions=['jinja2_time.TimeExtension'])
        #  autoescape=select_autoescape(['yaml']))
        # register filters and functions:
        self.env.filters['datetime'] = datetime
        self.env.filters['to_yaml'] = to_yaml
        self.env.filters['to_nice_yaml'] = to_nice_yaml
        self.env.filters['to_json'] = to_json
        self.env.filters['to_nice_json'] = to_nice_json
        self.env.globals['context']  = get_context
        self.env.globals['callable'] = callable

    def render_template(self,template_name,outfile_name,context):
        template = self.env.get_template(template_name)
        output = template.render(context)
        outfile_name = os.path.join(self.build_dir,outfile_name)
        with open(outfile_name, 'w') as of:
            of.write(output)
        return

    def fill_templates(self):
        dep = {}
        for v in self.topological_order:
            template =  self.g.vs[v]["template"]
            context =   self.g.vs[v]["transient"]
            outfile =   self.g.vs[v]["name"] # all anmes should be unique!

            try:
              self.render_template(template, outfile, context)
            except:
              self.logger.exception("Error rendering template '{:s}'!".format(template))
              raise

            dep[outfile] = [ self.g.vs[c]["name"] for c in self.g.neighbors(v,mode=igraph.OUT) ]

        with open(os.path.join(self.build_dir,"dependencies.yaml"), 'w') as f:
            yaml.dump(dep, f, default_flow_style=False)

        with open(os.path.join(self.build_dir,"metadata.yaml"), 'w') as f:
            yaml.dump(self.metadata, f, default_flow_style=False)

        with open(os.path.join(self.build_dir,"name.yaml"), 'w') as f:
            yaml.dump(self.name, f, default_flow_style=False)

        return

    def compile_workflow(self, outfile='wf.yaml', from_files=True):
        # negative fw ids:
        with open(os.path.join(self.build_dir,'dependencies.yaml'),'r') as stream:
            dependencies = yaml.safe_load(stream)
        with open(os.path.join(self.build_dir,'metadata.yaml'),'r') as stream:
            metadata = yaml.safe_load(stream)
        with open(os.path.join(self.build_dir,'name.yaml'),'r') as stream:
            wf_name = yaml.safe_load(stream)

        fws_set = set()
        for k,v in dependencies.items():
            fws_set.update([k,*v])
        self.logger.info("Set of Fireworks:{}.".format( sorted(fws_set) ) )

        fws_id = { fw: (-10*(i+1)) for i, fw in enumerate( sorted(fws_set) ) }

        fws_dict = {}
        for fw in sorted(fws_set):
            self.logger.info("Loading files '{}'.".format( fw ) )
            fws_dict[fw] = Firework.from_file(os.path.join(
                self.build_dir, fw), f_format='yaml')

        for name, fw in fws_dict.items():
            fw.fw_id = fws_id[name]
            self.logger.info("fw_id {: 4d}: {:s}".format(fw.fw_id, name))

        fws_list = list(fws_dict.values())

        links = {
            fws_dict[parent]: [
                fws_dict[child] for child in children ] for parent, children in dependencies.items() }

        wf = Workflow( fws_list, links, name=wf_name, metadata=metadata )
        wf.to_file(os.path.join(self.build_dir,outfile),f_format="yaml")

        return wf

    def find_undefined_variables(self):
        template_variables = { 'all' : set() }
        for template_name in self.env.list_templates():
            self.logger.info("Loading template {:s}.".format( template_name ) )
            template_source = self.env.loader.get_source(self.env,template_name)[0]
            try:
                parsed_content = self.env.parse(template_source)
            except:
                self.logger.exception("Failed parsing template '{:s}'".format(template_name))
                raise
            template_variables[template_name] = meta.find_undeclared_variables(parsed_content)
            template_variables['all'] = template_variables['all'] | template_variables[template_name]
        return template_variables

    def variable_overview(self, variables):
        lines = [ [ '', *variables['all'] ] ]
        for t,tv in variables.items():
            if t is not 'all':
                line = [ t ]
                line.extend( [ 'x' if v in tv else '' for v in variables['all'] ] )
                lines.append(line)
        return tabulate(lines,tablefmt='fancy_grid')

    def show_undefined_variables(self):
        """Show a table of all undefined variables in templates"""
        return self.variable_overview( self.find_undefined_variables() )

    def build_graph(self):
        fws_set = set()
        for k,vv in self.dependencies.items():
            v = [ list(v.keys())[0] if type(v) is dict else v for v in vv ]
            fws_set.update([k,*v])

        self.g = igraph.Graph(directed=True)
        self.g.add_vertices(sorted(list(fws_set)))
        self.logger.info("Graph contains {:d} nodes: {}.".format( len(self.g.vs), self.g.vs["name"] ) )

        #edges = [ (parent, child) for parent, children in self.dependencies.items() for child in children ]
        for parent, children in self.dependencies.items():
            for child in children:
                if type(child) is dict:
                    edge_type = list(child.values())[0] # expect only one entry
                    child     = list(child.keys())[0]
                else:
                    edge_type = 0 # default
                self.g.add_edge(parent,child,type=edge_type)
        self.logger.info("Graph contains {:d} edges: {}.".format( len(self.g.es),
            [ (self.g.vs[ self.g.es[e].source ]["name"], self.g.vs[ self.g.es[e].target ]["name"] ) for e in self.g.es.indices ] ) )

        #self.g.vs["instance"] = [[]]*len(self.g.vs)   # list of instances
        self.g.vs["persistent"] = [[{}]]*len(self.g.vs) # list of persistent contexts
        self.g.vs["transient"] = [[{}]]*len(self.g.vs)  # list of transient contextes
        for v in self.g.vs:
            if v["name"] in self.persistent_contexts:
                v["persistent"] = self.persistent_contexts[ v["name"] ]
            if v["name"] in self.transient_contexts:
                v["transient"] = self.transient_contexts[ v["name"] ]

        self.g.vs["template"] = self.g.vs["name"]  # this must not be touched later

        self.update()

    def update(self):
        if not self.g.is_dag():
            self.logger.exception("Graph is not DAG!")
            raise ValueError

        root = np.where(np.equal(self.g.degree(mode=igraph.IN),0))[0]
        if len(root) == 0:
            self.logger.exception("Graph has no root!")
            raise ValueError
        elif len(root) > 1:
            self.logger.exception("Graph root not unique: {}!".format( [ self.g.vs[v]["name"] for v in root ]))
            raise ValueError

        self.root = int(root[0])
        self.logger.info("Identified root {}.".format( self.g.vs[self.root]["name"] ) )

        self.g.vs["order"] = self.topological_order
        self.g.vs["dist"] = self.maximum_distance

        # for plotting purposes:
        # self.plt_layout = self.g.layout_auto()
        try:
            #self.plt_layout = self.g.layout_reingold_tilford(mode="out", root=[self.root]) # tree-like
            self.g.layout_fruchterman_reingold() # better than Kamanda-Kawai
        except:
            #self.plt_layout = self.g.layout_kamada_kawai()
            self.plt_layout = self.g.layout_sugiyama()

        self.g.vs["label"] = self.g.vs["name"]
        self.g.vs["label_size"] = self.plt_label_font_size


    def subgraph_at(self, v, x=set()):
        """Returns indices of all vertices in the subgraph beginning at node v, not following any edges leading to vertices in x"""
        if not isinstance(x, Iterable):
            x = set([x])

        visited = set()
        def dfs(v):
            visited.add(v)
            for child in self.g.neighbors(v,mode=igraph.OUT):
                if (child not in x) and (child not in visited):
                    dfs(child)
        dfs(v)
        return visited

    def subgraphs_below(self, v):
        """Returns list of list of indices of all vertices in the subgraphs beginning below node v"""
        forks = {}
        for eid in self.g.incident(v,mode=igraph.OUT):
            e = self.g.es[eid]
            s = self.subgraph_at(e.target)
            if e["type"] in forks:
                forks[ e["type"] ].update(s)
            else:
                forks[ e["type"] ] = set(s)

        return list(forks.values())

    def subgraph_except(self, x):
        """Returns indices of all vertices in the subgraph not following any path to any vertex in x"""
        visited = set()
        def dfs(v):
            visited.add(v)
            for child in self.g.neighbors(v,mode=igraph.OUT):
                if (child is not x) and (child not in visited):
                    dfs(child)
        dfs(self.root)
        return visited

        # visited has now all vertices of subgraph
        self.logger.info("Selected subgraph with nodes: {}.".format( internal ) )

    def duplicate_subset(self,sub,env,suffix='*'):
        """Duplicates subset within environment and returns old vid -> new vid dict mapping"""
        adjacent = set()

        def new_name(v):
            return (self.g.vs[v]["name"]+str(suffix))

        def duplicate_vertex(v):
            nam = new_name(v)
            self.h.add_vertex( nam )
            w = self.h.vs.find(name=nam).index
            self.logger.info("Added vertex {:d} - {:s}".format( w, nam ) )
            for attribute in self.g.vs.attribute_names():
                if attribute is not "name":
                    self.h.vs[w][attribute] = self.g.vs[v][attribute]
            self.h.vs[w]["label"] = self.h.vs[w]["name"]
            return w

        def duplicate_edge(e,s,t):
            self.h.add_edge(s,t)
            f = self.h.get_eid(s,t)
            self.logger.info("Added edge {:d} - ({:d}-{:d})".format( f, s, t ) )
            for attribute in self.g.es.attribute_names():
                self.h.es[f][attribute] = self.g.es[e][attribute]
            return f

        def edges_adjecent_to_subset(s):
            edges = set()
            for v in s:
                edges.update( self.g.incident(v,mode=igraph.ALL) )
            return edges

        dup = [ duplicate_vertex(v) for v in sub ]
        sub_to_dup = { s: d for s,d in zip(sub,dup) }

        edges = edges_adjecent_to_subset(sub)

        for e in edges:
            (source,target) = self.g.es[e].tuple

            if (source in sub) and (target in sub):
                duplicate_edge(e,sub_to_dup[source],sub_to_dup[target])
            elif (source in sub) and (target in env):
                duplicate_edge(e,sub_to_dup[source],target)
            elif (source in env) and (target in sub):
                duplicate_edge(e,source,sub_to_dup[target])

        return sub_to_dup

    def descend(self):
        """Descends the graph in topological order and creates forks where demanded

        Currently only supports bifurcations"""

        self.g.vs["visited"] = False

        # the topological order might change as new vertices are added during descend
        # introduced marker "visited" for this purpose
        suffix=''
        while True:
            vs = self.topological_order
            self.logger.info("Current topological order: {}".format( vs ) )

            for v in vs:
                #v = self.topological_order[i]
                if self.g.vs[v]["visited"] is True:
                    self.logger.info("{:d}: {} has been visited before, move on.".format(v, self.g.vs[v]["name"] ) )
                    continue

                self.logger.info("{:d}: {}.".format(v, self.g.vs[v]["name"] ) )

                self.g.vs[v]["visited"] = True
                #get all forks if vertex marked accordingly
                forks = self.subgraphs_below(v)

                # so far only supports bifurcation
                if len(forks) > 1:
                    self.logger.info("{:d} forks at v: {}.".format( len(forks), forks ) )

                    fl = forks.pop()
                    self.h = self.g.copy()
                    # TODO: implement support for > 2 fork
                    # use first fork as reference
                    # process forks pairwise:

                    fr = forks.pop()

                    self.logger.info("Lef and right fork: {}.".format( fl, fr ) )
                    overlap = fl & fr
                    self.logger.info("Overlap: {}.".format( overlap ) )


                    #env = set(self.topological_order) # a set of all nodes
                    env = set(self.subgraph_at(self.root)) # a set of all nodes
                    self.logger.info("Current set of vertices: {}.".format( env ) )

                    suffix = suffix + '*'

                    lenv = (env - fr ) | fl
                    self.logger.info("Environment of left fork: {}.".format( lenv ) )
                    self.duplicate_subset(overlap,lenv,suffix)

                    suffix = suffix + '*'

                    renv = (env - fl ) | fr
                    self.logger.info("Environment of right fork: {}.".format( lenv ) )

                    self.duplicate_subset(overlap,renv,suffix)

                    #to_delete.update(overlap)
                    self.h.delete_vertices(overlap)
                    self.g = self.h
                    self.update() # root index might change

                    # go up one level to while loop and rebuild topological order
                    # after modification of graph
                    self.logger.info("Forked and modified graph at node {}, rebuilding topological order.".format( v ) )
                    break
            else: # only if inner loop did not break:
                self.logger.info("All processed at node {}, finished.".format( v ) )
                break # all vertices v have been visited, also in extended graph
            self.logger.info("Completed processing node {}, descending.".format( v ) )
        return

    def maximum_spanning_tree(self):
        """Constructs maximum spanning tree by negation of weights"""
        #minimum_spanning_tree = self.g.spanning_tree()
        negated_weights = -np.ones(len(self.g.es))
        return self.g.spanning_tree(weights=negated_weights)

    def bfs(self):
        return self.g.bfs(self.root, mode = igraph.OUT)

    def bfs_advanced(self):
        bfs = self.g.bfsiter(self.root, mode = igraph.OUT, advanced = True)
        for (v, depth, p) in bfs:
            if p is not None:
                self.logger.info("{:03d}: {:s} - depth {:0d},  {:03d}: {:s}".format(v.index, v["name"], depth, p.index, p["name"] ))
            else:
                self.logger.info("{:03d}: {:s} - depth {:0d}, no ".format(v.index, v["name"], depth))

    def tree(self):
        """Yields tree with possibly degenerate vertices"""
        t, m = self.g.unfold_tree(self.root, mode=igraph.OUT)
        for v in t.vs:
            v["name"] = self.g.vs[ m[v.index] ]["name"]
        t.vs["label"] = t.vs["name"]
        t.vs["label_size"] = self.plt_label_font_size
        self.t = t
        return t

    def postorder(self):
        """Visit all vertices in post-order beginning at root"""
        visited = set()
        order = []
        def dfs(v):
            visited.add(v)
            for child in self.g.neighbors(v,mode=igraph.OUT):
                if child not in visited:
                    dfs(child)
            order.append(v)
        dfs(self.root)
        return order

    def select_closest_parent(self, v):
        """Returns parent closest above v, i.e. farthest from root"""
        parents = self.g.neighbors(v,mode=igraph.IN)
        selected_parent = None
        if len(parents) > 0:
            parent_distances = [ self.maximum_distance[p] for p in parents ]

            self.logger.info("{:d}: {:s}, dist. {:d}, top.pos. {:d} has {:d} parents with distances {}:".format(
               v, self.g.vs[v]["name"], self.g.vs[v]["dist"], self.g.vs[v]["order"], len(parents), parent_distances))
            for p in parents:
                self.logger.info("  {:d}: {:s}, dist. {:d}, top. pos. {:d}".format(
                    p, self.g.vs[p]["name"], self.g.vs[p]["dist"], self.g.vs[p]["order"]))

            # select indices of closest parents
            closest_parents = [ parents[p] for p in np.where( np.equal( np.max(parent_distances), parent_distances ) )[0] ]

            self.logger.info("    Maximum distance {:d} at parent {}".format(np.max(parent_distances),closest_parents) )

            if len(closest_parents) > 1:
                self.logger.warn("  {:d} parents are equally closest to child: Choice of context not unique.".format(len(closest_parents)))
                for p in closest_parents:
                    self.logger.warn("    {:d}: {:s}, dist. {:d}, top. pos. {:d}".format(
                        p, self.g.vs[p]["name"], self.g.vs[p]["dist"], self.g.vs[p]["order"]))
                self.logger.warn("  Choice of context not unique.")

            selected_parent = closest_parents[0]
            self.logger.info("  {:d}: {:s}, dist. {:d}, top. pos. {:d} selected as immediate parent".format(
                selected_parent, self.g.vs[selected_parent]["name"], self.g.vs[selected_parent]["dist"], self.g.vs[selected_parent]["order"]))
        else:
            self.logger.info("{:d}: {:s}, dist. {:d}, top.pos. {:d} has no parents.".format(v, self.g.vs[v]["name"], self.g.vs[v]["dist"], self.g.vs[v]["order"]))
        return selected_parent

    def build_degenerate_graph(self):
        # traverse in topological order to assure all parents have been processed before their children
        fwId = self.positive_fw_id_generator()

        self.g.vs["visited"] = False

        # self.h = igraph.Graph(directed=True)

        while True:
            vs = self.topological_order
            self.logger.info("Current topological order: {}".format( vs ) )

            env = self.subgraph_at(self.root) # the whole graph

            for v in vs:
                #v = self.topological_order[i]
                if self.g.vs[v]["visited"] is False:
                    break
                self.logger.info("{:d}: {} has been visited before, move on.".format(v, self.g.vs[v]["name"] ) )
            else:
                break # all v visted

            self.logger.info("{:d}: {}.".format(v, self.g.vs[v]["name"] ) )
            self.logger.info("Iterating node {:d}: {:s}".format(v, self.g.vs[v]["name"] ))

            self.g.vs[v]["visited"] = True
            #get all forks if vertex marked accordingly
            #forks = self.subgraphs_below(v)
            sub = self.subgraph_at(v) # all vertices below v

            fw_class_name = self.g.vs[v]["template"]
            if fw_class_name not in self.env.list_templates():
                    raise ValueError("No template '{:s}' exists!".format(fw_class_name))
            # self.logger.warn("{}{}:".format(' '*depth, ))

            # children = g.neighbors(v,mode=igraph.OUT)
            # select  with largest distance to root as immediate predecessor
            # this is not well defined for parents on the same level
            # parents = self.g.neighbors(v,mode=igraph.IN)

            parent_id = self.select_closest_parent(v)
            if parent_id is not None:
                # parent_instances = self.h.vs.select(instanceOf=parent_id)
                parent_context = self.h.vs[parent_id]["persistent"] # inherit context
                parent_degeneracy = self.h.vs[parent_id]["degeneracy"]
                parent_name = self.h.vs[parent_id]["name"]
            else: # root node
                parent_context = self.std_context
                parent_degeneracy = 1
                parent_name = None

            persistent_context_updates = self.g.vs[v]["persistent"]
            transient_context_updates = self.g.vs[v]["transient"]

            degeneracy = parent_degeneracy*len(persistent_context_updates)*len(transient_context_updates)

            self.logger.info("Degeneracy (persistent*transient): {:d}*{:d} = {:d}".format(
                len(persistent_context_updates),len(transient_context_updates),degeneracy))

            self.logger.debug("  Parent name: {}".format(parent_name))
            self.logger.debug("  Parent context: {}".format(parent_context))
            self.logger.debug("  Child persistent context updates {}".format(persistent_context_updates))
            self.logger.debug("  Child transient context updates {}".format(transient_context_updates))
            # take all  instances and first apply ...

            # return zip(parent_names, parent_contexts)
            # if degeneracy > 1
                # for (parent_name, parent_context) in zip(parent_names, parent_contexts):

            self.logger.info("  Iterating parent {}:".format(parent_name))

            self.h = self.g.copy()
            for i, persistent_context_update in enumerate(persistent_context_updates):
                self.logger.info("    Iterating persitent content update {:d}:".format(i))
                # ... persitent changes ...
                persistent_context = parent_context.copy()
                persistent_context.update( persistent_context_update )
                self.logger.debug("      Current persistent context: {}".format(persistent_context))

                for j, transient_context_update in enumerate(transient_context_updates):
                    self.logger.info("      Iterating transient content update {:d}:".format(j))
                    # then apply transient changes ...
                    transient_context = persistent_context.copy()
                    transient_context.update( transient_context_update )

                    self.logger.debug("        Current transient context: {}".format(transient_context))

                    fw_id = next(fwId) # get unique id

                    suffix = "_{:06d}".format(fw_id)
                    child_name = "{:s}{:s}".format(fw_class_name,suffix)
                    sub_to_dup = self.duplicate_subset(sub, env, suffix)

                    # update properties of new subgraph root
                    self.h.vs[ sub_to_dup[v] ]["name"] = child_name
                    self.h.vs[ sub_to_dup[v] ]["persistent"] = persistent_context
                    self.h.vs[ sub_to_dup[v] ]["transient"]  = transient_context
                    self.h.vs[ sub_to_dup[v] ]["degeneracy"] = degeneracy
                    self.h.vs[ sub_to_dup[v] ]["fw_id"]      = fw_id
                    # self.h.add_vertex(
                    #    child_name,
                    #    instanceOf=v,
                    #    fw_id=fw_id,
                    #    persistent=persistent_context,
                    #    transient=transient_context)
                    # child_id = self.h.vs.find(name=child_name).index

                    self.logger.info("          Instance {:d} - {:d} - {:s} of class {:d} - {:s} created.".format(
                        sub_to_dup[v],
                        fw_id,
                        child_name,
                        v,
                        self.g.vs[v]["template"] ) )
                    #if parent_name is not None:
                    #    self.h.add_edge(parent_name, child_name)
                    #    parent_id = h.vs.find(name=parent_name).index
                    #    self.logger.info("          Attached to instance {:d} - {:d} - {:s} of class {:d}: {:s}.".format(
                    #        parent_id,
                    #        h.vs[parent_id]["fw_id"],
                    #        h.vs[parent_id]["name"],
                    #        h.vs[parent_id]["instanceOf"],
                    #        self.g.vs[ h.vs[parent_id]["instanceOf"] ]["name"]))
                    #to_delete.update(overlap)

                    # go up one level to while loop and rebuild topological order
                    # after modification of graph
                    self.logger.info("      Completed transient content update {:d}:".format(j))

                self.logger.info("    Completed persitent content update {:d}:".format(i))
            #self.logger.info("  Completed parent {}:".format(parent_name))

            self.h.delete_vertices(sub)
            self.g = self.h
            self.update() # root index might have changed

            self.logger.info("      Modified graph at node {}, rebuilding topological order.".format( v ) )
            self.logger.info("Completed node {:d}: {:s}".format(v, self.g.vs[v]["name"] ) )

        self.logger.info("All processed at node {}, finished.".format( v ) )

        return

    def plot(self, g = None):
        if g is None:
            g = self.g
        return igraph.plot(g, layout = self.plt_layout, bbox = self.plt_bbox, margin = self.plt_margin)

    def show_attributes(self, vs=None, exclude=[
        "name", "persistent", "transient", "label", "label_size", "visited"] ):
        if vs is None:
            vs = self.g.vs
        else:
            vs = self.g.vs.select(vs)

        return tabulate(
            [
                [ '', "name", *[a for a in vs.attribute_names() if a not in exclude] ],
                *list(
                    zip(
                        [ i for i in range(len(vs)) ],
                        vs["name"],
                        *[ vs[a] for a in vs.attribute_names() if a not in exclude ] ) ) ] )

# modified default arguments will persist until the next call!!!
def extract_partial_workflow(wf, fw_id = None, wfs_dict = None,
    fws_dict = None, fws_set = None, links = None ):
    """Recursively extracts partial Workflow at certain Fireworks
    (and all sub WF as well)

    Args:
      wf (Workflow):  the original workflow
      fw_id (int):    the node of interest, root if default: None,
                      more than one root not allowed then
      wfs_dict ( {int: Workflow }): dict of (FW IDs, sub-workflows), hand empty
                      reference to retrieve all sub-w'flows, default: None
      fws_dict ( {int: Firework }): automatically built, leave at default: None
      fws_set ( set( Firework ) ):  automatically built, hand empty reference to
                      retrieve sub-set of FW IDs in partial w'flow
      links:  ( {int: [int]} ): automatically built, hand empty reference to
                      retrieve sub-dict of parent-child links in partial w'flow
    Returns:
      Workflow:       partial w'flow built at fw_id

    """
    global logger

    if wfs_dict is None: wfs_dict = {}
    if fws_set  is None: fws_set = set()
    if links    is None: links = {}

    if fws_dict is None:
        fws_dict = { fw.fw_id: fw for fw in wf.fws }
        logger.debug(
            "FWs in original WF: {}".format(fws_dict) )

    if fw_id is None:
        assert len(wf.root_fw_ids) == 1, "More than one root fws!"
        fw_id = wf.root_fw_ids[0]

    assert isinstance(fw_id, int), "Only integer FW IDs allowed!"

    fws_set.add(fw_id)
    links.update({fw_id: wf.links[fw_id]})

    for child_fw_id in links[fw_id]:
        child_fws_set = set()
        child_links   = {}
        # create a new workflow for every child
        child_wf = extract_partial_workflow(
            wf, fw_id=child_fw_id, wfs_dict=wfs_dict, fws_dict=fws_dict,
            fws_set=child_fws_set, links=child_links)

        # if child has not yet been visited before, update set of all sub-WFs
        if child_fw_id not in wfs_dict:
            logger.debug(
                "First visit at FW {}: {}".format(
                child_fw_id, fws_dict[child_fw_id].name ) )
            wfs_dict.update( {child_fw_id: child_wf} )

        fws_set.update( child_fws_set )
        links.update( child_links )

    # reconstruct all FW in partial w'flow from FW IDs in subset:
    fws_list = [ fws_dict[fw_id] for fw_id in fws_set ]

    # partial workflow is named after its root FW, metadata same as original WF
    return Workflow( fws_list, links,
        name=fws_dict[fw_id].name, metadata=wf.metadata )

def build_wf(system_infile = 'system.yaml', build_dir = 'build', template_dir = 'templates'):
    """Build workflow from system .yaml description and set of templates.

    Args:
        system_infile (str): .yaml description of system. (default: system.yaml)
        build_dir (str): output directory for rendered templates. (default: build)
        template_dir (str): directory containing jinja2 template set. (default: templates)

    Returns:
        Nothing.
    """
    wfb = WorkflowBuilder(system_infile)
    wfb.template_dir = template_dir
    wfb.build_dir = build_dir
    wfb.initialize_template_engine()
    try:
        undefined_variables_output = wfb.show_undefined_variables()
    except Exception as e:
        print(e)
        error = e
        raise

    ### Conversion to tree with degenerate vertices
    wfb.descend()
    wfb.build_degenerate_graph()
    show_attributes_output = wfb.show_attributes()

    ## Build Workflow
    try:
        wfb.fill_templates()
    except Exception as e:
        print(e)
        error = e
        raise

    try:
        wf = wfb.compile_workflow()
    except Exception as e:
        print(e)
        error = e
        raise

    return
